---
title: "Model fitting"
author: 
    - Katie Schuler
date: 2023-10-05
---

::: {.callout-warning title="Under Construction"}
:::

```{r}
#| echo: false
#| message: false
library(tidyverse)
library(modelr)
library(infer)
library(knitr)
theme_set(theme_classic(base_size = 20))

# setup data 
data <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(1.2, 2.5, 2.3, 3.1, 4.4)
)

data2 <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(2, 2.5, 3.3, 4.1, 6.4)


)

```

```{r}
# Set the seed for reproducibility
set.seed(42)

# Generate 500 random values for x between 1 and 6
x <- rnorm(500, mean = 3.5, sd = 1)

# Create y with a correlation to x, adding some noise
y <- 0.8 * x + rnorm(500, mean = 0, sd = 0.5)

# Combine x and y into a data frame
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.5) + 
    geom_smooth(method = "lm", se = FALSE) +
    annotate(x = 1.2, y = 6, geom = "text", label = "● Data", size = 6) + 
    annotate(x = 1.2, y = 5.5, geom = "text", label = "--- Model", size = 6, color = "blue") + 
    annotate("text", x = 5.5, y = 1.0, label = "model specification", size = 6) +
      annotate("text", x = 5.5, y = 0.4, label = expression(y == a * x + b), size = 6)


```



1. **Start with the Basic Idea of Linear Regression**:
   - Remind students that linear regression is about finding the best-fitting line through a set of points on a graph. The goal is to find the coefficients (slope and intercept) that minimize the differences between the observed values and the values predicted by the line.

2. **Introduce the Concept of Error**:
   - Explain that we measure how well our line fits the data using something called "error" (or "residuals"). The error is the difference between the actual values and the predicted values.
   - Introduce the idea of the sum of squared errors (SSE) as a way to quantify this error:
     \[
     \text{SSE} = \sum (y_i - \hat{y}_i)^2,
     \]
     where \( y_i \) are the actual values and \( \hat{y}_i \) are the predicted values from our line.

3. **Understanding the Goal**:
   - State that our goal is to **find the line (or coefficients)** that make the SSE as small as possible. This is like playing a game where you want to get the lowest score.

4. **Introduce the Idea of Finding the Minimum**:
   - Rather than using calculus terms, you can simply say that to find the best line, we want to adjust the slope and intercept until the SSE is at its lowest point. 
   - Explain that the **"minimum"** means that if you change the slope or intercept a little bit, the SSE will start to increase again. It's like finding the lowest point in a valley.

5. **Explain the Role of the Derivative in Simple Terms**:
   - Describe the derivative as a tool that helps us understand how changes in the slope and intercept affect the SSE. 
   - You can say: "Think of it as a way to see which direction we should move our line to make the SSE smaller. If the derivative is positive, it means that increasing the slope or intercept is making the SSE larger, so we need to go the other way. If it's negative, we know that increasing it will make the SSE smaller."

6. **Introduce the Closed-Form Solution**:
   - Transition to how we can find the exact values for the slope and intercept without guessing or iterating. 
   - Present the formula for the closed-form solution:
     \[
     \boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.
     \]
   - Explain that this formula gives us a direct way to calculate the best-fitting line based on our data. It’s like using a recipe where we combine certain ingredients (the data) to get the perfect dish (the coefficients).

7. **Visualize and Relate**:
   - Use a graph to show how the line fits the data points, emphasizing that the closed-form solution provides the exact line that minimizes the error.
   - You can also relate it to a familiar process, like solving a simple equation: “Just like in algebra where we can find the solution for \( x \) directly, here we can find the best slope and intercept directly without needing to guess.”

By simplifying the explanation of the derivative and focusing on the intuition behind finding the minimum error, students can grasp the essential concepts of linear regression without getting lost in calculus.



Great idea! Here’s how you can smoothly integrate gradient descent into your explanation after introducing the sum of squared errors (SSE) and the concept of a loss function:

1. **Introduce the Loss Function**:
   - After explaining the SSE, frame it as a **loss function**, which measures how well our model (the line) is performing. The loss function is what we aim to minimize to improve our model's predictions.

2. **Explain the Concept of Optimization**:
   - State that we want to find the values of the slope and intercept that result in the smallest possible value of this loss function (SSE). This is the optimization goal in linear regression.

3. **Introduce Gradient Descent**:
   - Now, explain that one common way to find those optimal values is by using a method called **gradient descent**. 
   - Say something like: "Imagine we are standing on a hill and want to find the lowest point in a valley. The gradient tells us the steepness and direction of the hill. In our case, gradient descent helps us figure out how to change our slope and intercept to move towards the lowest SSE."

4. **Describe How Gradient Descent Works**:
   - Explain that in gradient descent, we start with some initial guesses for the slope and intercept (these could be random values).
   - Then, we repeatedly adjust these values based on the gradient (the direction and steepness of the loss function). Each adjustment is like taking a step down the hill, getting us closer to the minimum:
     - For each iteration:
       1. Calculate the SSE with the current slope and intercept.
       2. Determine the gradient (how much the SSE changes with small changes to the slope and intercept).
       3. Update the slope and intercept by taking a step in the opposite direction of the gradient.

5. **Use Simple Visuals**:
   - You can visualize gradient descent with a graph showing the SSE as a curve, illustrating how each step brings the slope and intercept closer to the minimum point on that curve.

6. **Connect Gradient Descent and the Closed-Form Solution**:
   - After explaining gradient descent, connect it back to the closed-form solution: "While gradient descent helps us find the best-fitting line through repeated adjustments, the closed-form solution gives us a direct calculation to achieve the same result. Both methods ultimately lead to the best slope and intercept, just through different approaches."

7. **Reinforce the Concept**:
   - Emphasize that both methods (gradient descent and the closed-form solution) are valid ways to solve the same problem. This will help students understand that there are multiple approaches to optimization in linear regression.

By structuring the explanation this way, you create a logical flow from the concept of error and loss functions to the iterative nature of gradient descent, making it easier for students to grasp both ideas.

## You are here

## Model building 

## Linear model review

- how would we specify this model 
- how would we write it in R 

- estimate the free paramters 

## Estimate free parameters

```{r}

# with base R 
lm(y ~ 1 + x, data = data)


# with infer workflow 
data %>%
    specify(y ~ 1 + x) %>%
    fit()


```



## Model fitting basics


Linear model 

```{r}
#| echo: false
#| layout-ncol: 2
data <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(1.2, 2.5, 2.3, 3.1, 4.4)
)

data2 <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(2, 2.5, 3.3, 4.1, 6.4)


)

print(data)

ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred")


```

We can see which fits better with our eyes. 

```{r}
#| echo: false
#| layout-ncol: 2
#| layout-nrow: 3



ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE) +
coord_cartesian(ylim = c(0, 7)) +
labs(tag = "A", title = "Fits well")


ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(
    data = data2, 
    mapping = aes(x = x, y = y), 
    method = "lm", formula = "y ~ x", se = FALSE) +
    coord_cartesian(ylim = c(0, 7)) +
labs(tag = "B", title = "Fits poorly")

```

```{r}
#| echo: false
#| layout-ncol: 2
#| layout-nrow: 3



ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE) +
coord_cartesian(ylim = c(0, 7)) +
labs(tag = "A", title = "Low MSE")


ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(
    data = data2, 
    mapping = aes(x = x, y = y), 
    method = "lm", formula = "y ~ x", se = FALSE) +
    coord_cartesian(ylim = c(0, 7)) +
labs(tag = "B", title = "High MSE")

modelA <- lm(y ~ x, data = data)
modelB <- lm(y ~ x, data = data2)

mseA <- data %>% add_predictions(modelA) %>%
    mutate(err = y - pred, sq_err = err^2)
kable(mseA)

mseB <- data %>% add_predictions(modelB) %>%
    mutate(err = y - pred, sq_err = err^2)
kable(mseB)



```


```{r}
mean(mseA$sq_err)
mean(mseB$sq_err)
```

```{r}
#| echo: false
#| layout-ncol: 2

ggplot(mseA, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE) +
geom_segment(aes(xend = x, yend = pred)) +
coord_cartesian(ylim = c(0, 7))  +
labs(tag = "A", title = "Low MSE")



ggplot(mseB, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(
    data = data2, 
    mapping = aes(x = x, y = y), 
    method = "lm", formula = "y ~ x", se = FALSE) +
    geom_segment(aes(xend = x, yend = pred)) +
    coord_cartesian(ylim = c(0, 7)) +
labs(tag = "B", title = "High MSE")



```






```{r}

lm(y ~ 1 + x, data = data)

data %>%
    specify(y ~ 1 + x) %>%
    fit()




```


```{r}
b0 <- seq(from = 0, to = 3, by = 0.1)
b1 <- seq(from = 0, to = 3, by = 0.1)
possible_weights <- expand.grid(b0 = b0, b1 = b1)

ggplot(data = possible_weights, 
    mapping = aes(x = b0, y = b1)) +
    geom_point()


# compute the sum of squares for those weights on a dataframe
sum_squares <- function(b0, b1) {

    data %>% 
        mutate(pred = b0 + b1*x) %>%
        mutate(err = pred-y) %>%
        mutate(sq_err = err^2) %>%
        select(sq_err) %>%
        sum()
   
}
error_surf <- possible_weights %>% 
    rowwise() %>%
    mutate(sum_sq =  sum_squares(b0, b1)) %>%
    ungroup

error_surf

error_surf  %>% filter(sum_sq < 0.608)

ggplot(error_surf, aes(b0, b1, z = sum_sq)) +
    geom_contour_filled()

    
        
        # %>%
        # sum(.$sq_err)
       # summarise(sum_sq = sum(sq_err)) %>%


# return sum of squares as a column next to 

# mse <- function(data, b0, b1) {
#   model_value <- b0 + b1*data[1]
#   resid <- data[2] - model_value
#   sq_err <- resid^2
#   sum(sq_err)
# }

# possible_weights %>% mutate(
#     mse = mse(1, 1, b0, b1)
# )

```

- in context of model building more broadly 
- a genear overview of the concept 

## Mean squared error 

Cost function. 

## Error surface

- We can visualize the error surface for simple example: 2 parameters, $\beta_0$ and $\beta_1$, and the cost function (mean square error). 
- Show nonlinear model v linear model figs 
- goal is to find the minimum point
- notice the nonlinear model can have local minimums but lm has only 1. Because lm is a **convex** function. 

## Gradient descent 

IF we want to estimate the free parameters in a way that would work broadly, for linear or nonlinear models, we can use **gradient descent**. 

- machine learning / optimization. 
- If we have a lot of data, we could use **stochastic gradient descent** which is the same except we... 

## Ordinary least squares 

As we saw above, linear models have the special property that they have a solution, the OLS. Rather than searching the error surface iteratively via gradient descent (optimization), we can solve for this point directly with **linear algebra**. 

- matrix approach: we write the 3-step function. 
- use lm() in R. 
- infer approach: 
    - specify(), fit() 



### Further reading 

- [Ch. 8 Fitting models to data](https://dtkaplan.github.io/SM2-bookdown/fitting-models-to-data.html) in Statistical Modeling



