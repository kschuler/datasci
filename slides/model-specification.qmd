---
title: "Week 07: Model Specification"
subtitle: "Data Science for Studying Language and the Mind"
author: Katie Schuler
date: 2024-09-17
echo: true
format: 
    revealjs:
        chalkboard: true
        slide-number: true
        incremental: true 
        footer: "[https://kathrynschuler.com/datasci](https://kathrynschuler.com/datasci/)"
---

```{r}
#| echo: false
library(tidyverse)
library(infer)
library(languageR)
theme_set(theme_classic(base_size = 20))
set.seed(123)

```

# Tuesday

## Announcements {.smaller}

:::: {.columns}

::: {.column width="50%"}

![](/assests/images/exam-1-scores.png)
:::

::: {.column width="50%"}


- You did great on the exam! 
- You can replace your lowest exam score with the optional final
- The final exam is cumulative: another opportunity to show mastery of the material.
:::
::::

## Thanks for your feedback {.smaller}

. . . 

:::: {.columns}

::: {.column width="50%"}
### Adding
1. **Demos more accessible**
  - Posted before class 
  - Make font bigger
  - Not so fast, please üòÖ
2. **In-class exercises** (not graded)
  - Slightly more interactive 
3. **Challenge questions** 
  - On labs or homework (optional)
:::

::: {.column width="50%"}
### Not adding
4. **Projects** instead of exams
5. **R Studio** instead of Google Colab
:::
::::


## You are `here` {.smaller} 

:::: {.columns}

::: {.column width="33%"}

##### Data science with R 
::: {.nonincremental}
- R basics
- Data visualization
- Data wrangling
:::
:::

::: {.column width="33%"}

##### Stats & Model building
::: {.nonincremental}
- Sampling distribution
- Hypothesis testing
- `Model specification`
- Model fitting 
- Model accuracy
- Model reliability
:::
:::

::: {.column width="33%"}

##### More advanced 
::: {.nonincremental}

- Classification
- Inference for regression
- Mixed-effect models
::: 
:::

::::


# Review 

Sampling distribution and hypothesis testing with Correlation!

## Exploring relationships {.smaller}

To review what we learned before break, let's explore the relationship between Frequency and meanFamiliarity in the `ratings` dataset of the `languageR` package. 

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ratings %>% 
  ggplot(aes(x = Frequency, y = meanFamiliarity))  +
    geom_point()
```

## Is there a relationship?  {.smaller}

If there was no relationship, we'd say there are **independent**: knowing the value of one provides no information about about the other. But that's not the case here.

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ratings %>% 
  ggplot(aes(x = Frequency, y = meanFamiliarity))  +
    geom_point()
```

## Yes, a linear relationship {.smaller}

In a linear relationship, when one variable goes up the other goes up (positive); or when one goes up the other goes down (negative). 

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ratings %>% 
  ggplot(aes(x = Frequency, y = meanFamiliarity))  +
    geom_point() +
    geom_smooth(color = "blue", method = "lm", se = FALSE)
```

## Quantify with correlation {.smaller}

One way to quantify linear relationships is with **correlation** ($r$). Correlation expresses the linear relationship as a range from -1 (perfectly negative) to 1 (perfectly positive). 

![](/assests/images/correlation.jpeg)

## Computing correlation in R {.smaller}

We can compute a correlation with R's built in `cor(x,y)` function

. . . 

```{r}
#| output-location: column

cor(
  x = ratings$Frequency, 
  y = ratings$meanFamiliarity
)
```

. . . 

Or via the `infer` pacakge. 

. . . 

```{r}
#| output-location: column

(obs_corr <- ratings %>%
  specify(
    response = meanFamiliarity, 
    explanatory = Frequency
  ) %>%
  calculate(stat = "correlation"))
```

## Correlation uncertainty {.smaller}

Just like the mean ‚Äî and all other test statistics! ‚Äî $r$ is subject to sampling variability. We can indicate our uncertainty around the correlation the same way we always have:

. . . 

Construct the sampling distribution for the correlation: 

. . . 
```{r}
#| output-location: column

sampling_distribution <- ratings %>%
  specify(
    response = meanFamiliarity, 
    explanatory = Frequency
  ) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "correlation")

head(sampling_distribution)
```

. . . 

Compute a confidence interval 

. . . 

```{r}
#| output-location: column

ci <- sampling_distribution %>% 
  get_ci(level = 0.95, type = "percentile")

ci
```

## üí™ In-class Exercise 7.1 {.smaller}

*Take a few minutes to try this yourself!*

Use the `infer` way to visualize the sampling distribution and shade the confidence interval we just computed. Change the x-axis label to **stat (correlation)** as pictured below. 

. . . 

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 5

sampling_distribution %>%
  visualize() + 
  shade_ci(ci) +
  labs(x = "stat (correlation)")
```

## Hypothesis testing our correlation {.smaller}

How do we test whether the correlation we observed is significantly different from zero? Hypothesis test! 

. . . 

Step 1: Construct the null distribution, the sampling distribution of the null hypothesis

. . . 

```{r}
#| output-location: column

null_distribution_corr <- ratings %>% 
  specify(
    response = meanFamiliarity, 
    explanatory = Frequency
  ) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "correlation") 

null_distribution_corr %>% head
```

. . . 

Step 2: How likley is our observed value under the null? Get a p-value.

. . . 

```{r}
#| output-location: column

p <- null_distribution_corr %>%
  get_p_value(
    obs_stat = obs_corr, 
    direction = "both")
p
```


## Hypothesis testing our correlation {.smaller}

How do we test whether the correlation we observed is significantly different from zero? Hypothesis test! 

. . . 

Step 3: Decide whether to reject the null! 

. . . 

```{r}
#| output-location: column

null_distribution_corr %>% 
  visualize() + 
  shade_p_value(
    obs_stat = obs_corr, 
    direction = "two-sided"
  ) 
```

. . . 

Interpret our p-value. Should we reject the null hypothesis?

# Model building 

Big picture overview of the model building process and the types of models we might encounter in our research. 

## Correlation is model building {.smaller}

Correlation is a simple case of model building, in which we use one value ($x$) to predict another ($y$). 

. . . 

```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ggplot(ratings, aes(x = Frequency, y = meanFamiliarity)) +
    geom_point(alpha = 1) +
    annotate(x = 2.2, y = 6, geom = "text", label = "‚óè Data", size = 6) + 
    labs(x = "Frequency \n x", y = "y \n meanFamiliarity") +
    annotate(x = 7.5, y = 2.3, geom = "text", label = expression(r == 0.482), size = 6, color = "red")
    

```


## Correlation is model building {.smaller}

Even more specifically ‚Äî formally, the **model specification** ‚Äî we are fitting the linear model $y = ax+b$, where $a$ and $b$ are free parameters. 

:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false 
#| fig-width: 5
#| fig-height: 5

ggplot(ratings, aes(x = Frequency, y = meanFamiliarity)) +
    geom_point(alpha = 1) +
    geom_smooth(color = "blue", method = "lm", se = FALSE) +
    annotate(x = 2.2, y = 6, geom = "text", label = "‚óè Data", size = 6) + 
    labs(x = "Frequency \n x", y = "y \n meanFamiliarity") + 
    annotate(x = 2.2, y = 5.5, geom = "text", label = "--- Model", size = 6, color = "blue") + 
    annotate(x = 7.5, y = 2.3, geom = "text", label = expression(r == 0.482), size = 6, color = "red") 

```

::: 
::: {.column width=50%}

- Model specification: $y = ax + b$ 
- Estimate free parameters: $a$ and $b$
- Fitted model: $y = 0.39x + 2.02$

:::
::::

## How do we get $r = 0.48$ ? {.smaller}

The link between correlation and linear models is understood when we normalize our variables with a z-score. 

```{r}
#| echo: true
#| output-location: column

z_ratings <- ratings %>%
  select(Frequency, meanFamiliarity) %>%
  mutate(
    z_Freq = scale(Frequency), 
    z_meanFamil = scale(meanFamiliarity)
  )

z_ratings %>% head

```

- A z-score gets the number of standard deviations a data point is from the mean.

## Correlation is the slope of the model {.smaller}

Correlation is the slope of the line that best predicts $y$ from $x$ (after z-scoring)
```{r}
#| echo: false 
#| layout-ncol: 2
#| fig-height: 10

ggplot(ratings, aes(x = Frequency, y = meanFamiliarity)) +
    geom_point(alpha = 1) +
    geom_smooth(color = "blue", method = "lm", se = FALSE) +
    annotate(x = 7.5, y = 2.3, geom = "text", label = expression(r == 0.482), size = 10, color = "red") + 
    annotate(x = 7.0, y = 2.0, geom = "text", label = expression(y == 0.39*x + 2.02), size = 10, color = "blue") +
    labs(title = "raw data") +
    theme_classic(base_size = 30)

ggplot(z_ratings, aes(x = z_Freq, y = z_meanFamil)) +
    geom_point(alpha = 1) +
    geom_smooth(color = "red", method = "lm", se = FALSE) +
    annotate(x = 2, y = -1.5, geom = "text", label = expression(r == 0.482), size = 10, color = "red") +
    annotate(x = 1.75, y = -1.8, geom = "text", label = expression(y == 0.482*x + 0), size = 10, color = "blue") +
    labs(title = "z scored data") +
    theme_classic(base_size = 30)

```




## Model building overview 

- `Model specification` (this week): specify the functional form of the model.
- `Model fitting`: you have the form, how do you estimate the free parameters? 
- `Model accuracy`: you've estimated the parameters, how well does that model describe your data? 
- `Model reliability`: when you estimate the parameters, you want to quantify your uncertainty on your estimates 


## Types of models {.smaller}

. . . 


```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";

  "supervised learning" [style = filled]

}
``` 

## üí™ In-class Exercise 7.2 {.smaller}

*Take a few minutes to try this yourself!*

Ask ChatGPT what type of model it is made with? 

. . . 

![](/assests/images/chatgpt-exercise.png)

## Supervised learning {.smaller} 

. . . 

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model [style = "invis"];
  x2 -> model [style = "invis"];
  x3 -> model [style = "invis"];
  x4 -> model [style = "invis"];
  x5 -> model [style = "invis"];

  model -> y [style = "invis"];  // Connect the model to y
}

```

## Supervised learning {.smaller}

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model;
  x2 -> model;
  x3 -> model;
  x4 -> model;
  x5 -> model;

  model -> y [style = "invis"];  // Connect the model to y
}

```

## Supervised learning {.smaller}

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model;
  x2 -> model;
  x3 -> model;
  x4 -> model;
  x5 -> model;

  model -> y;  // Connect the model to y
}

```

## Types of models 

```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";
  "supervised learning" -> "regression";
  "supervised learning" -> "classification";
 
  

  "regression" [style = filled]

}
```


## Regression v classification {.smaller}

```{dot}
//|echo: false

digraph G {

rankdir=LR;  // Left to Right layout

  // Column 1: x values
  x1 [label="x1"];
  x2 [label="x2"];
  x3 [label="x3"];

  // Column 2: model
  model [label="Model", shape=box, width=1.5, height=1, style = filled];

  // Column 3: y value
  y [label="y"];

  // Column 4: regression v classify 
  r [label = "Regression (y is continuous) \n 1 3 4 2 3 4", shape = box];
  c [label = "Classification (y is discrete) \n yes/no, male/female/nonbinary", shape = box]; 

  // Keep x values in the same rank
  { rank=same; x1; x2; x3; }

  // Keep alignment using invisible edges
  x1 -> model;
  x2 -> model;
  x3 -> model;
  x4 -> model;
  x5 -> model;

  model -> y;  // Connect the model to y
  y -> r; 
  y -> c; 
}

```

## Types of models 

```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";
  "supervised learning" -> "regression";
  "supervised learning" -> "classification";
  "regression" -> "linear models";
  "regression" -> "nonlinear models";
  

  "linear models" [style = filled]

}
```

## Linear models 




:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false 
#| fig-height: 10

# Set seed for reproducibility
set.seed(42)

# Generate 500 random values for x
x <- rnorm(500, mean = 0, sd = 1)  # You can adjust the mean and sd as needed

# Calculate y based on the equation y = 2x + 3, and add noise
noise <- rnorm(500, mean = 0, sd = 1)  # You can adjust the sd of the noise
y <- 2 * x + 3 + noise

# Combine x and y into a data frame
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 3) +
    #labs(title = "Linear model") +
    theme_classic(base_size = 40) 

```

::: 
::: {.column width=50%}

- **Linear models** are models in which the output (y) is a weighted sum of the inputs 
- $y=\sum_{i=1}^{n}w_ix_i$ 
- $y = ax + b$ is this!
- Easy to understand and fit


:::
::::

## $y = ax + b$ as $y=\sum_{i=1}^{n}w_ix_i$ 




:::: {.columns}
::: {.column width="50%"}


```{r}
#| echo: false 
#| fig-height: 10

# Set seed for reproducibility
set.seed(42)

# Generate 500 random values for x
x <- rnorm(500, mean = 0, sd = 1)  # You can adjust the mean and sd as needed

# Calculate y based on the equation y = 2x + 3, and add noise
noise <- rnorm(500, mean = 0, sd = 1)  # You can adjust the sd of the noise
y <- 2 * x + 3 + noise

# Combine x and y into a data frame
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 3) +
    #labs(title = "Linear model") +
    theme_classic(base_size = 36) 

```

::: 
::: {.column width=50%}

- implicit constant: $y=ax+b\mathbf{1}$  
- let $x_1=x$ and $x_2=\mathbf{1}$ 
- $y=w_1x_1 + w_2x_2$ where $w_1$ and $w_2$ are free parameters

:::
::::


## Types of models 

```{dot}
//| echo: false

digraph G {
    
  "models" -> "supervised learning";
  "models" -> "unsupervised learning";
  "supervised learning" -> "regression";
  "supervised learning" -> "classification";
  "regression" -> "linear models";
  "regression" -> "nonlinear models";
  

  "nonlinear models" [style = filled]

}
```

## Nonlinear models {.smaller}

Output (y) cannot be expressed as a weighted sum of inputs($y=\sum_{i=1}^{n}w_ix_i$ ); pattern is better captured by more complex functions. (But often we can linearize them!)


```{r}
#| echo: false 
#| layout-ncol: 2
#| fig-height: 10

# Set seed for reproducibility
set.seed(42)

# Generate 500 random values for x
x <- rnorm(500, mean = 0, sd = 1)  # You can adjust the mean and sd as needed

# Calculate y based on the equation y = 2x + 3, and add noise
noise <- rnorm(500, mean = 0, sd = 1)  # You can adjust the sd of the noise
y <- 2 * x + 3 + noise

# Combine x and y into a data frame
data <- data.frame(x = x, y = y)

ggplot(data, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 3) +
    labs(title = "Linear model") +
    theme_classic(base_size = 36) 

# Generate x values (input variable) from 0 to 6 with 500 data points
x <- rnorm(500, mean = 0, sd = 1)

# Define the quadratic model y = ax^2 + bx + c with random noise
a <- 1.2   # Coefficient for x^2
b <- -0.5  # Coefficient for x
c <- 3.0   # Constant term
noise <- rnorm(length(x), mean = 0, sd = 1)  # Random noise

# Calculate y values (response variable)
y <- a * x^2 + b * x + c + noise

# Create a data frame with x, "Model", and y
data_nonlinear <- data.frame(x = x, Model = "Model", y = round(y, 2))

ggplot(data_nonlinear, aes(x = x, y = y)) +
    geom_point(alpha = 0.5, size = 4) +
    geom_smooth(method = "lm", se = FALSE, formula = "y ~ poly(x, 2)", linewidth = 3) +
    labs(title = "Nonlinear model") +
    theme_classic(base_size = 36)

```

## üí™ In-class Exercise 7.3 {.smaller}

*Take a few minutes to try this yourself!*

Load the following data, which shows brain size and body weight for several different animals. 

- [animal_brain_body_size.csv](/assests/csv/animal_brain_body_size.csv)

Explore the data to specify the type of model we should use to predict brain size by body weight. 

- Supervised or unsupervised? 
- Regression or classification?
- Linear or nonlinear? 





# To be continued ... 

on Thursday 